<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Modeling of Active Anti-Roller Bars</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, Helvetica, sans-serif;
            margin: 0px;
            padding: 0px;
            padding-left: 100px;
            padding-top: 30Px;
            padding-right: 100px;
            background-color: black;
            color: white;
            line-height: 24px;
            cursor: none;
            text-align: justify;
        }
        div.bigspace{
            line-height: 44px;
        }
        h1{
            color: rgb(192, 76, 11);
        }
        h2{
            color: rgb(192, 76, 11);
        }
        h3{
            color: rgb(192, 76, 11);
        }
        .center {
          display: block;
          margin-left: auto;
          margin-right: auto;
          width: 50%;
        }
        
    </style>
</head>
<body>
    
    <header>
        <h1>Basic Autonomous Driving System</h1>
    </header>

    <main>
        <p>In this project we are aiming to design and implement a basic Autonomous driving system that can navigate 
            a simulated environment in Carla Simulator. The system should be capable of lane following, obstacle avoidance
             and traffic sign recognition. The main aim of the project is to simulate a vehicle that can safely navigate
              its surrounding based on the real time sensor data. We have created different strategy to achieve the results, 
              first we have started with the semantic segmentation to classify road, sidewalk, vehicles and pedestrian. 
              We will extract the lane marking from the road and try to extract the curvature of the road in order to adjust 
              the steering angle. Secondly, we have attempted to create the birdâ€™s eye view using the four cameras attach on
               the four side of the view using the CNN model. Third model is to detect the vehicles, road, sidewalk and pedestrian 
               using the Lidar attached at the top of the Ego vehicle, the CNN model takes the 3D tensor of size of defined voxel
                (volumetric data) and give 2D output containing the categorical representation of each class. Also, traffic signal
                 and sign detection are incorporated to control the vehicle accordingly. This project not only limited to the Machine
                  leaning algorithm but also required image processing, control system design and basic decision theory as well 
                  to achieve the results.
        </p>

        <h2>Model</h2>
        <h3>U-Net Architecture</h3>
        <p>We used the U-Net architecture, a Convolutional Neural Network (CNN) built especially for pixel-wise 
            segmentation, to accomplish the semantic segmentation tasks in this study. U-Net is very effective 
            for tasks requiring exact boundary predictions, including identifying roads, vehicles, and pedestrians 
            in an autonomous driving system, because of its encoder-decoder structure and skip links, which allow
             it to absorb both local and global contextual information. </p>
        <img src="unet.png" style="width: 30%;height: auto" class="center">
        <h3>VoxelCNN</h3>
        <p>Volumetric voxelized LiDAR data is processed by the second Convolutional Neural Network (CNN)
             used in this research, VoxelCNN, which classifies each voxel into the following categories: 
             road, sidewalk, cars, pedestrians, and background. This model uses a CNN-based architecture 
             that works on the voxelized 3D space to effectively manage the 3D structure of LiDAR point
              clouds. A 4D tensor of shape [batch size, channels, depth, height, breadth] is produced by 
              the VoxelCNN after processing LiDAR data.
        </p>
        <img src="voxelcnn.png" style="width: 40%;height: auto" class="center">
        <h3>Faster RCNN</h3>
        <p>
            We used Faster R-CNN, a popular two-stage object identification model, to identify signs and traffic
             signals in the simulated environment. This design is excellent at locating things using bounding boxes, 
             detecting them, and classifying them into predetermined categories.
        </p>
        <img src="rcnn.png" style="width: 30%;height: auto" class="center">

        <h2>Results</h2>
        <h3>U-Net Architecture: Semantic Segmentation Results</h3>
        <img src="semsegDemo.gif" style="width: 60%;height: auto" class="center">
        <h3>VoxelCNN: LiDAR Segmentation Results</h3>
        <img src="lidarDemo.gif" style="width: 60%;height: auto" class="center">
        <h3>Faster RCNN: Traffic Signal Detection Results</h3>
        <img src="signalDemo.gif" style="width: 80%;height: auto" class="center">
        <h3>Final Combination of Models</h3>
        <img src="finalDemo.gif" style="width: 60%;height: auto" class="center">
        <h3>Raining Scenario Demo</h3>
        <img src="rainDemo.gif" style="width: 60%;height: auto" class="center">

        <br><br>

    </main>
    <link rel="stylesheet" href="../../styles/cursor.css">
    <div id="cursor" class="Cursor"></div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/3.12.2/gsap.min.js"></script>
    <script src="../../scripts/cursor.js"></script>
</body>
</html>
